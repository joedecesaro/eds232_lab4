---
title: "lab4c - iNaturalist"
author: "Joe DeCesaro"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# load libraries
librarian::shelf(
  digest, dplyr, DT, readr, tidyr, keras, stringr, glue, purrr)

```

## Set Up
```{r}
# path to folder containing species directories of images
dir_src  <- "/courses/EDS232/inaturalist-2021/train_mini"
dir_dest <- "~/EDS232/eds232_lab4/inat"
dir.create(dir_dest, showWarnings = F)

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_src, recursive = F, full.names = T)
n_spp <- length(dirs_spp)

# set seed (for reproducible results) 
# just before sampling (otherwise get different results)
# based on your username (unique amongst class)
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible 
i10
```


```{r}
# show the 10 species directory names
basename(dirs_spp)[i10]
```

```{r}
# show the first 2 species directory names
i2 <- i10[1:2]
basename(dirs_spp)[i2]
```

```{r}
# setup data frame with source (src) and destination (dest) paths to images
d <- tibble(
  set     = c(rep("spp2", 2), rep("spp10", 10)),
  dir_sp  = c(dirs_spp[i2], dirs_spp[i10]),
  tbl_img = map(dir_sp, function(dir_sp){
    tibble(
      src_img = list.files(dir_sp, full.names = T),
      subset  = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  unnest(tbl_img) %>% 
  mutate(
    sp       = basename(dir_sp),
    img      = basename(src_img),
    dest_img = glue("{dir_dest}/{set}/{subset}/{sp}/{img}"))

# show source and destination for first 10 rows of tibble
d %>% 
  select(src_img, dest_img)
```

```{r}
# iterate over rows, creating directory if needed and copying files 
d %>% 
  pwalk(function(src_img, dest_img, ...){
    dir.create(dirname(dest_img), recursive = T, showWarnings = F)
    file.copy(src_img, dest_img) })

# uncomment to show the entire tree of your destination directory
# system(glue("tree {dir_dest}"))
```

## 2 Species (binary classification)
###  Data Processing

```{r}
train_dir_sp2 <- paste0(dir_dest,"/spp2/train")
validation_dir_sp2 <- paste0(dir_dest,"/spp2/validation")
test_dir_sp2 <- paste0(dir_dest,"/spp2/test")
```

```{r, warning=FALSE}
# All images will be rescaled by 1/255
train_datagen_sp2 <- image_data_generator(rescale = 1/255)
validation_datagen_sp2 <- image_data_generator(rescale = 1/255)
test_datagen_sp2 <- image_data_generator(rescale = 1/255)

# For 2 species set up generators
train_generator_sp2 <- flow_images_from_directory(
  # This is the target directory
  train_dir_sp2,
  # This is the data generator
  train_datagen_sp2,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 5,
  # Since we use binary_crossentropy loss, we need binary labels
  class_mode = "binary")

validation_generator_sp2 <- flow_images_from_directory(
  validation_dir_sp2,
  validation_datagen_sp2,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")

test_generator_sp2 <- flow_images_from_directory(
  test_dir_sp2,
  test_datagen_sp2,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")
```

###  Neural Net

```{r, warning=FALSE}
model_sp2_nn <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model_sp2_nn %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy"))
```

```{r}
model_sp2_nn %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = 0.0001),
  loss = "binary_crossentropy",
  metrics = c("accuracy"))
```

```{r}
history_sp2_nn <- model_sp2_nn %>% fit(
    train_generator_sp2,
    steps_per_epoch = 6,
    epochs = 20,
    validation_data = validation_generator_sp2,
    validation_steps = 1)
```

```{r}
plot(history_sp2_nn)
```

```{r}
results_sp2_nn <- model_sp2_nn %>% evaluate(test_generator_sp2)
results_sp2_nn
```

### Convolutional Neural Net

```{r}
cnn_model_sp2 <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")  

 
cnn_model_sp2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 0.0001),
  metrics = c("acc"))
```

```{r}
# train the model
history_cnn_sp2 <- cnn_model_sp2 %>% fit(
  train_generator_sp2,
  steps_per_epoch = 6,
  epochs = 20,
  validation_data = validation_generator_sp2,
  validation_steps = 1)
str(history_cnn_sp2)
```

```{r}
plot(history_cnn_sp2)
```

```{r}
results_cnn_sp2 <- cnn_model_sp2 %>% evaluate(test_generator_sp2)
```

### Comparing Standard Neural Net and Convolutional Neural Net

```{r}
results_sp2_nn

results_cnn_sp2

```

The standard neural net has an accuracy of `r round(results_sp2_nn[[2]], 3)` and a loss rate of `r round(results_sp2_nn[[1]], 3)`. The convolutional neural net has an accuracy of `r round(results_cnn_sp2[[2]], 3)` and a loss rate of `r round(results_cnn_sp2[[1]], 3)`. Based on these results for the 2 species classification I don't think there is enough of a difference to choose between the two models. I ran the models several times and it would switch between the two models as to which was performing better.

## 10 Species (multi-class classification) 
### Data Processing

```{r}
# new directories
train_dir_sp10 <- paste0(dir_dest,"/spp10/train")
validation_dir_sp10 <- paste0(dir_dest,"/spp10/validation")
test_dir_sp10 <- paste0(dir_dest,"/spp10/test")
```

```{r}
# All images will be rescaled by 1/255
train_datagen_sp10 <- image_data_generator(rescale = 1/255)
validation_datagen_sp10 <- image_data_generator(rescale = 1/255)
test_datagen_sp10 <- image_data_generator(rescale = 1/255)

train_generator_sp10 <- flow_images_from_directory(
  train_dir_sp10,
  train_datagen_sp10,
  target_size = c(150, 150),
  batch_size = 5,
  # categorical class because we have more than 2 species
  class_mode = "categorical")

validation_generator_sp10 <- flow_images_from_directory(
  validation_dir_sp10,
  validation_datagen_sp10,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")

test_generator_sp10 <- flow_images_from_directory(
  test_dir_sp10,
  test_datagen_sp10,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")
```

### Neural Net

```{r}
model_sp10_nn <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(150, 150, 3)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 10, activation = "softmax")
```

```{r}
model_sp10_nn %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = 0.0001),
  loss = "categorical_crossentropy",
  metrics = c("accuracy"))
```

```{r}
# train our model
history_sp10_nn <- model_sp10_nn %>% fit(
  train_generator_sp10,
  steps_per_epoch = 5,
  epochs = 30,
  validation_data = validation_generator_sp10,
  validation_steps = 10)
```

```{r}
plot(history_sp10_nn)
```

```{r}
results_sp10_nn <- model_sp10_nn %>% evaluate(test_generator_sp10)
```

### Convolutional Neural Net

```{r}
cnn_model_sp10 <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")
```

```{r}
cnn_model_sp10 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 0.0001),
  metrics = c("acc"))
```

```{r}
# train the model
history_cnn_sp10 <- cnn_model_sp10 %>% fit(
  train_generator_sp10,
  steps_per_epoch = 5,
  epochs = 30,
  validation_data = validation_generator_sp10,
  validation_steps = 10)
```

```{r}
plot(history_cnn_sp10)
```

```{r}
results_cnn_sp10 <- cnn_model_sp10 %>% evaluate(test_generator_sp10)
```

### Comparing Standard Neural Net and Convolutional Neural Net

```{r}
results_sp10_nn

results_cnn_sp10
```

The standard neural net has an accuracy of `r round(results_sp10_nn[[2]], 3)` and a loss rate of `r round(results_sp10_nn[[1]], 3)`. The convolutional neural net has an accuracy of `r round(results_cnn_sp10[[2]], 3)` and a loss rate of `r round(results_cnn_sp10[[1]], 3)`. Based on these results for the 10 species classification I don't think there is enough of a difference to choose between the two models. I ran the models several times and it would switch between the two as to which was performing better.


